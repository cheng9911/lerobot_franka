# streamlit run lerobot/resnet/task_think.py --server.address=192.168.1.14
import streamlit as st
import json
import subprocess
import platform
import os
from openai import OpenAI
import asyncio
import edge_tts
import threading
import tempfile
from gtts import gTTS
import pyttsx3


def say(text, blocking=False):
    system = platform.system()

    def play_audio(mp3_path):
        if system == "Darwin":
            # macOS ç”¨ afplay æ’­æ”¾
            subprocess.run(['afplay', mp3_path])
        elif system == "Linux":
            # Linux ç”¨ mpg123 æ’­æ”¾
            subprocess.run(['mpg123', '-q', mp3_path])
        elif system == "Windows":
            # Windows ç”¨ powershell æ’­æ”¾
            ps_command = f'(New-Object Media.SoundPlayer "{mp3_path}").PlaySync();'
            subprocess.run(['powershell', '-Command', ps_command], shell=True)
        else:
            print("Unsupported OS for audio playback.")

    def run():
        with tempfile.NamedTemporaryFile(suffix=".mp3", delete=False) as f:
            mp3_path = f.name
        try:
            tts = gTTS(text=text, lang='zh')
            tts.save(mp3_path)
            play_audio(mp3_path)
        finally:
            if os.path.exists(mp3_path):
                os.remove(mp3_path)

    if blocking:
        run()
    else:
        threading.Thread(target=run, daemon=True).start()
print("å¼€å§‹ä»»åŠ¡")
# say("å¼€å§‹ä»»åŠ¡", blocking=True)


# ç”¨äºè¯­éŸ³åé¦ˆï¼ˆå¯é€‰ï¼‰
# def say(text, blocking=False):
#     system = platform.system()
#     if system == "Darwin":
#         cmd = ['say', text]
#     elif system == "Linux":
#         cmd = ['espeak-ng', '-v', 'cmn-latn-pinyin', text]
#     elif system == "Windows":
#         ps_command = (
#             f'Add-Type -AssemblyName System.Speech;'
#             f'(New-Object System.Speech.Synthesis.SpeechSynthesizer).Speak("{text}")'
#         )
#         cmd = ['powershell', '-Command', ps_command]
#     else:
#         return
#     subprocess.run(cmd) if blocking else subprocess.Popen(cmd)

# åˆå§‹åŒ– LLM
client = OpenAI(
    api_key="sk-p4fUo6CD8qznKOLC8bQARL1BfcaulU7XCta9H0IJDNrAOXh3",  # æ›¿æ¢ä¸ºä½ çš„ Moonshot Key
    base_url="https://api.moonshot.cn/v1",
)

def detect_intent(user_input: str):
    """è°ƒç”¨å¤§æ¨¡å‹åˆ¤æ–­æ„å›¾ï¼šä»»åŠ¡æŒ‡ä»¤ or èŠå¤©"""
    prompt = f"""
ä½ æ˜¯ä¸€ä¸ªæœºå™¨äººåŠ©æ‰‹å°æ™ºåŒå­¦ï¼Œè¯·åˆ¤æ–­ä»¥ä¸‹ç”¨æˆ·è¾“å…¥æ˜¯å±äºâ€œæœºå™¨äººä»»åŠ¡è§„åˆ’æŒ‡ä»¤â€è¿˜æ˜¯æ™®é€šâ€œèŠå¤©æé—®â€ï¼š

è¾“å…¥ï¼š{user_input}

è¯·è¿”å›ä¸€ä¸ªJSONå¯¹è±¡ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{{
  "intent": "task" æˆ– "chat",
  "reason": "ä½ åšå‡ºåˆ¤æ–­çš„ç†ç”±"
}}
"""
    response = client.chat.completions.create(
        model="moonshot-v1-8k",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.2,
        max_tokens=200
    )
    content = response.choices[0].message.content.strip()
    try:
        json_start = content.find("{")
        return json.loads(content[json_start:])
    except:
        return {"intent": "chat", "reason": "æ„å›¾è¯†åˆ«å¤±è´¥ï¼Œé»˜è®¤å½“ä½œèŠå¤©"}

def load_predicted_skills(path="lerobot/resnet/data/llm/predicted_skills.json"):
    if not os.path.exists(path):
        return []
    with open(path, "r", encoding="utf-8") as f:
        data = json.load(f)
        return data[0]["skill_sequence"]

def call_image_skill_predictor():
    try:
        subprocess.run(
            ["python", "lerobot/resnet/predict_image_skill.py", "--num-predicts", "1"],
            check=True
        )
    except Exception as e:
        st.error(f"å›¾åƒæŠ€èƒ½è¯†åˆ«å¤±è´¥: {e}")
        return []

    return load_predicted_skills()

def generate_prompt(task_description, predicted_skills):
    skill_dict = {
        0: "æŠ“å–ç†ŠçŒ«ï¼ˆé»‘ç™½ï¼‰æ”¾åˆ°ç›’å­",
        1: "æŠ“å–å…”å­ï¼ˆç²‰çº¢è‰²ï¼‰æ”¾åˆ°ç›’å­",
        2: "æŠ“å–è€è™ï¼ˆé»„è‰²ï¼‰æ”¾åˆ°ç›’å­",
        3: "æŠ“å–èœ˜è››ä¾ ï¼ˆçº¢è‰²ï¼‰æ”¾åˆ°ç›’å­",
        4: "æŠ“å–å…¬ä»”ï¼ˆè“è‰²ï¼‰æ”¾åˆ°ç›’å­",
        5: "æ— æ•ˆ/é‡å¤æŠ€èƒ½"
    }

    readable_skills = [skill_dict.get(s, f"æœªçŸ¥æŠ€èƒ½{s}") for s in predicted_skills if s != 5]

    prompt = f"""
ä½ æ˜¯æœºå™¨äººä»»åŠ¡è§„åˆ’åŠ©æ‰‹å°æ™ºåŒå­¦ï¼Œç°åœ¨æœ‰å¦‚ä¸‹ä»»åŠ¡ï¼š
- ä½ éœ€è¦æ ¹æ®ç”¨æˆ·ç”¨è‡ªç„¶è¯­è¨€æè¿°çš„é«˜å±‚ä»»åŠ¡ï¼Œç»“åˆå½“å‰ç¯å¢ƒçŠ¶æ€ï¼Œæ¨ç†å‡ºæœ€åˆç†çš„æŠ€èƒ½æ‰§è¡Œåºåˆ—ã€‚è¾“å‡ºç»“æ„åŒ–çš„JSONæ ¼å¼çš„æŠ€èƒ½è§„åˆ’ç»“æœã€‚
- åŒæ—¶ä½ å…·å¤‡å¯¹è¯èƒ½åŠ›ï¼Œèƒ½ç”¨è‡ªç„¶è¯­è¨€è§£é‡Šè§„åˆ’é€»è¾‘ï¼Œå›ç­”ç”¨æˆ·çš„ç–‘é—®ï¼ˆæ¯”å¦‚â€œä½ æ˜¯è°ï¼Œå½“å‰ç©å¶æœ‰ä»€ä¹ˆâ€ï¼‰ã€‚è¯¥ç±»å‹åˆ¤å®šä¸ºinfo
- ä½ ç†è§£æŠ€èƒ½åº“çš„èƒ½åŠ›è¾¹ç•Œï¼Œå–„äºå¤„ç†å¼‚å¸¸æƒ…å†µï¼Œç¡®ä¿è§„åˆ’åˆç†ä¸”å¯æ‰§è¡Œã€‚
- è¾“å‡ºçš„å›ç­”å¿…é¡»åŒ…å«ï¼šJSONæ ¼å¼çš„ä»»åŠ¡è§„åˆ’ç»“æœï¼ŒåŒ…å«å­—æ®µï¼š"status"ï¼ˆsuccess/error/infoï¼‰ã€"message"ï¼ˆç®€æ˜æç¤ºï¼‰ã€"planned_skills"ï¼ˆæŠ€èƒ½åˆ—è¡¨ï¼ŒæˆåŠŸæ—¶éç©ºï¼Œå¤±è´¥æˆ–ä¿¡æ¯æ—¶å¯ç©ºï¼‰ã€‚
-è¾“å‡ºæ›´åŠ æ‹Ÿäººï¼ŒåƒçœŸäººä¸€æ ·å›å¤
-å¦‚æœæ£€æµ‹åˆ°ä¸æ­¢ä¸€ä¸ªæŠ€èƒ½ç¼–å·ï¼Œæ ¹æ®ç”¨æˆ·çš„æé—®ï¼ŒåŒ¹é…æœ€åˆé€‚çš„æŠ€èƒ½æ”¾åˆ°planned_skillsï¼Œ0: "æŠ“å–ç†ŠçŒ«ï¼ˆé»‘ç™½ï¼‰æ”¾åˆ°ç›’å­",
        1: "æŠ“å–å…”å­ï¼ˆç²‰çº¢è‰²ï¼‰æ”¾åˆ°ç›’å­",
        2: "æŠ“å–è€è™ï¼ˆé»„è‰²ï¼‰æ”¾åˆ°ç›’å­",
        3: "æŠ“å–èœ˜è››ä¾ ï¼ˆçº¢è‰²ï¼‰æ”¾åˆ°ç›’å­",
        4: "æŠ“å–å…¬ä»”ï¼ˆè“è‰²ï¼‰æ”¾åˆ°ç›’å­",
        5: "æ— æ•ˆ/é‡å¤æŠ€èƒ½"
- ç”¨æˆ·æè¿°ä»»åŠ¡ä¸ºï¼š{task_description}
- å½“å‰å›¾åƒè¯†åˆ«å‡ºçš„å¯è¡ŒæŠ€èƒ½ä¸ºï¼š{readable_skills}

è¯·åˆ¤æ–­æ˜¯å¦å¯ä»¥æ‰§è¡Œè¯¥ä»»åŠ¡ï¼Œä¸¥æ ¼è¾“å‡ºä»¥ä¸‹JSONæ ¼å¼ï¼š
{{
  "status": "success|error|info",
  "message": "å¯¹è§„åˆ’ç»“æœçš„è§£é‡Š",
  "planned_skills"ï¼ˆæŠ€èƒ½åˆ—è¡¨ï¼Œæ ¼å¼ä¸ºå­—ç¬¦ä¸²æ•°ç»„ï¼Œæ¯é¡¹ä¸ºâ€œç¼–å·: æŠ€èƒ½åç§°â€ï¼Œä¾‹å¦‚ï¼š"0: æŠ“å–ç†ŠçŒ«ï¼ˆé»‘ç™½ï¼‰æ”¾åˆ°ç›’å­"),
  "explanation": "è¿™é‡Œæ˜¯ç»™ç”¨æˆ·çš„è¯¦ç»†è‡ªç„¶è¯­è¨€è¯´æ˜",
}}
è¯·ä¸è¦è¾“å‡º JSON ä»¥å¤–çš„ä»»ä½•æ–‡æœ¬ã€‚
"""
    return prompt
import re
def extract_json(text):
    # æå–ç¬¬ä¸€ä¸ªå®Œæ•´çš„ JSON å¯¹è±¡
    match = re.search(r"\{[\s\S]*?\}", text)
    if match:
        return json.loads(match.group())
    else:
        raise ValueError("æ‰¾ä¸åˆ°æœ‰æ•ˆçš„ JSON å†…å®¹")

def call_task_planner(task_description, predicted_skills):
    prompt = generate_prompt(task_description, predicted_skills)
    response = client.chat.completions.create(
        model="moonshot-v1-8k",
        messages=[
            {"role": "system", "content": "ä½ æ˜¯æœºå™¨äººä»»åŠ¡è§„åˆ’åŠ©æ‰‹å°æ™ºåŒå­¦ã€‚"},
            {"role": "user", "content": prompt}
        ],
        temperature=0.1,
        max_tokens=600,
    )
    answer = response.choices[0].message.content.strip()
    try:
        # json_start = answer.find("{")
        # return json.loads(answer[json_start:])
        return extract_json(answer)
    except Exception as e:
        return {
  "status": "error",
  "message": f"è§£æå¤±è´¥ï¼š{str(e)}",
  "raw_response": answer
}
        # return {"status": "error", "message": f"è§£æå¤±è´¥ï¼š{str(e)}", "raw_response": answer}

def call_chat_response(user_input):
    """éä»»åŠ¡å‹å¯¹è¯æ—¶çš„å›ç­”"""
    response = client.chat.completions.create(
        model="moonshot-v1-8k",
        messages=[
            {"role": "system", "content": "ä½ æ˜¯æœºå™¨äººåŠ©æ‰‹å°æ™ºï¼Œå…·å¤‡äº²å’ŒåŠ›å’Œå¯¹è¯èƒ½åŠ›ï¼Œèƒ½è§£é‡Šå½“å‰ç³»ç»Ÿæƒ…å†µã€å›¾åƒçŠ¶æ€ã€æŠ€èƒ½ç­‰"},
            {"role": "user", "content": user_input}
        ],
        temperature=0.3,
        max_tokens=500
    )
    return response.choices[0].message.content.strip()

# ---------- Streamlit ä¸»ç•Œé¢ ----------
st.set_page_config(page_title="æœºå™¨äººä»»åŠ¡è§„åˆ’å™¨", layout="centered")
st.title("ğŸ¤– æœºå™¨äººæ™ºèƒ½åŠ©æ‰‹å°æ™º")

task_description = st.text_input("è¯·è¾“å…¥ä½ çš„é—®é¢˜æˆ–ä»»åŠ¡ï¼š", "å¸®æˆ‘æ•´ç†ç†ŠçŒ«")

if st.button("å‘é€"):
    with st.spinner("ğŸ” æ­£åœ¨åˆ†æè¾“å…¥æ„å›¾..."):
        intent_result = detect_intent(task_description)
        st.info(f"åˆ¤å®šä¸ºï¼š{intent_result['intent']}  ğŸ¯ï¼ˆ{intent_result['reason']}ï¼‰")

    if intent_result["intent"] == "task":
        with st.spinner("ğŸ§  æ­£åœ¨æ ¹æ®ç¯å¢ƒåŒ¹é…æŠ€èƒ½..."):

            predicted_skills = call_image_skill_predictor()

        if predicted_skills:
            with st.spinner("ğŸ¤– æ­£åœ¨è¿›è¡Œä»»åŠ¡è§„åˆ’..."):
                result = call_task_planner(task_description, predicted_skills)
                st.subheader("ğŸ“‹ ä»»åŠ¡è§„åˆ’ç»“æœï¼š")
                st.json(result)
                say(result.get("message", "ä»»åŠ¡å·²è§„åˆ’å®Œæˆ"), blocking=True)
                # ä» planned_skills æå–ç¼–å·ï¼ˆæ”¯æŒ "0:xxx" å­—ç¬¦ä¸²æˆ–çº¯æŠ€èƒ½åï¼‰
                def extract_skill_ids_from_list(skills):
                    ids = []
                    for s in skills:
                        if isinstance(s, str):
                            match = re.match(r"(\d+)\s*:", s)
                            if match:
                                ids.append(int(match.group(1)))
                    return ids

                # ç¼–å· â†’ æŒ‡ä»¤æ˜ å°„
                skill_command_map = {
                    0: [
                        "python", "lerobot/scripts/gello_test.py", "record",
                        "--fps", "30", "--root", "data",
                        "--repo-id", "SunJincheng/gello_model",
                        "--tags", "tutorial", "eval",
                        "--warmup-time-s", "12", "--episode-time-s", "120",
                        "--reset-time-s", "30", "--num-episodes", "2",
                        "--force-override", "1",
                        "-p", "outputs/train/act_panda_dish/checkpoints/030000/pretrained_model"
                    ],
                    # å¯æ·»åŠ æ›´å¤šæŠ€èƒ½ç¼–å·åŠå‘½ä»¤
                }

                def execute_skill_by_id(skill_id,planned_skills):
                    command = skill_command_map.get(skill_id)
                    if command:
                        try:
                            st.info(f"ğŸ› ï¸ æ­£åœ¨æ‰§è¡ŒæŠ€èƒ½ {planned_skills}...")
                            subprocess.run(command, check=True)
                            st.success("âœ… æŠ€èƒ½æ‰§è¡Œå®Œæˆã€‚")
                        except subprocess.CalledProcessError as e:
                            st.error(f"âŒ æŠ€èƒ½æ‰§è¡Œå¤±è´¥ï¼š{e}")
                    else:
                        st.warning(f"âš ï¸ æœªçŸ¥æŠ€èƒ½ç¼–å·ï¼š{skill_id}")

                # æ‰§è¡Œé˜¶æ®µ
                planned_skills = result.get("planned_skills", [])
                print("Planned skills:", planned_skills)
                skill_ids = extract_skill_ids_from_list(planned_skills)

                if skill_ids:
                    for skill_id in skill_ids:
                        execute_skill_by_id(skill_id,planned_skills)
                else:
                    st.warning("æœªæå–åˆ°æŠ€èƒ½ç¼–å·ï¼Œè·³è¿‡æ‰§è¡Œé˜¶æ®µã€‚")
                
        else:
            st.error("æœªæ£€æµ‹åˆ°å¯ç”¨æŠ€èƒ½ï¼Œä»»åŠ¡è§„åˆ’å–æ¶ˆã€‚")
    else:
        with st.spinner("ğŸ’¬ æ­£åœ¨å›å¤ä½ çš„é—®é¢˜..."):
            response = call_chat_response(task_description)
            st.markdown(response)
            say(response)

with st.expander("ğŸ“˜ ä½¿ç”¨è¯´æ˜"):
    st.markdown("""
    è¾“å…¥å¯ä»¥æ˜¯ä»»åŠ¡ç±»æŒ‡ä»¤ï¼ˆå¦‚â€œæ•´ç†ç†ŠçŒ«â€ï¼‰ï¼Œä¹Ÿå¯ä»¥æ˜¯æ™®é€šæé—®ï¼ˆå¦‚â€œä½ æ˜¯è°â€ï¼‰ã€‚
    ç³»ç»Ÿä¼šè‡ªåŠ¨è¯†åˆ«æ„å›¾ï¼Œè°ƒç”¨åˆé€‚çš„å¤„ç†æ–¹å¼ï¼š
    - **ä»»åŠ¡ç±»** â†’ å›¾åƒè¯†åˆ« + æŠ€èƒ½è§„åˆ’
    - **èŠå¤©ç±»** â†’ æ™ºèƒ½åŠ©æ‰‹ç›´æ¥å›å¤
    """)
